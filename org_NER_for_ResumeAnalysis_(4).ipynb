{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluevit/Recruit-Right-Precision-Hiring-with-AI-Insight/blob/main/org_NER_for_ResumeAnalysis_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRF5YRktW07P"
      },
      "source": [
        "<h3>Training spaCy NER for Resume Analysis</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOYpCT9v9RBd",
        "outputId": "67de9238-0ad6-480b-a963-6f1a40b43a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy #==1.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv_CRv5cm7Hx",
        "outputId": "8b12502f-5720-4b66-ceed-fa7665996fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.6)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.0.2)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (4.49.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (0.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.31.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy #==3.7.2\n",
        "!pip install spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgpoPgHMnl1F"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOytCEpK13eM",
        "outputId": "9b9cc221-6132-4a39-e30f-d4d0127d48e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-trf==3.8.0) (0.3.0)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_trf  # Download Transformer-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39pJf9gNnWu3",
        "outputId": "03153841-50d2-40f5-9382-42bc503079be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.6)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.0.2)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (4.49.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (0.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.31.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy-transformers\n",
        "import spacy\n",
        "from spacy.tokens import DocBin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPitVvbJ2EV0",
        "outputId": "21fbd2a1-5731-4cfc-f8b0-da3dedbdbb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final corrected config.cfg written.\n"
          ]
        }
      ],
      "source": [
        "config_text = \"\"\"[paths]\n",
        "train = \"./train_data.spacy\"\n",
        "dev = \"./dev_data.spacy\"\n",
        "vectors = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"transformer\", \"ner\"]\n",
        "\n",
        "[components]\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.transformer]\n",
        "factory = \"transformer\"\n",
        "\n",
        "[training]\n",
        "dropout = 0.2\n",
        "patience = 5\n",
        "max_epochs = 20\n",
        "seed = 42\n",
        "gpu_allocator = \"pytorch\"\n",
        "accumulate_gradient = 1\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "annotating_components = []\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "before_to_disk = null\n",
        "before_update = null\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "learn_rate = 0.00005\n",
        "\n",
        "[training.score_weights]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.cfg\", \"w\") as f:\n",
        "    f.write(config_text)\n",
        "\n",
        "print(\"✅ Final corrected config.cfg written.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap7YCy2E2HEG",
        "outputId": "c28a6446-fb24-4196-98dc-ea8897c955fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[paths]\n",
            "train = \"./train_data.spacy\"\n",
            "dev = \"./dev_data.spacy\"\n",
            "vectors = null\n",
            "\n",
            "[nlp]\n",
            "lang = \"en\"\n",
            "pipeline = [\"transformer\", \"ner\"]\n",
            "\n",
            "[components]\n",
            "[components.ner]\n",
            "factory = \"ner\"\n",
            "\n",
            "[components.transformer]\n",
            "factory = \"transformer\"\n",
            "\n",
            "[training]\n",
            "dropout = 0.2\n",
            "patience = 5\n",
            "max_epochs = 20\n",
            "seed = 42\n",
            "gpu_allocator = \"pytorch\"\n",
            "accumulate_gradient = 1\n",
            "max_steps = 20000\n",
            "eval_frequency = 200\n",
            "frozen_components = []\n",
            "annotating_components = []\n",
            "dev_corpus = \"corpora.dev\"\n",
            "train_corpus = \"corpora.train\"\n",
            "before_to_disk = null\n",
            "before_update = null\n",
            "\n",
            "[training.logger]\n",
            "@loggers = \"spacy.ConsoleLogger.v1\"\n",
            "\n",
            "[training.batcher]\n",
            "@batchers = \"spacy.batch_by_words.v1\"\n",
            "discard_oversize = false\n",
            "tolerance = 0.2\n",
            "[training.batcher.size]\n",
            "@schedules = \"compounding.v1\"\n",
            "start = 100\n",
            "stop = 1000\n",
            "compound = 1.001\n",
            "\n",
            "[training.optimizer]\n",
            "@optimizers = \"Adam.v1\"\n",
            "learn_rate = 0.00005\n",
            "\n",
            "[training.score_weights]\n"
          ]
        }
      ],
      "source": [
        "!cat config.cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glaUB9zsolEf",
        "outputId": "17b62ae8-8ca0-424f-c5ee-08605d10e163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CV-Parsing-using-Spacy-3' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git\n",
        "\n",
        "# !git clone https://github.com/Mehyarmlaweh/NER-Annotated-CVs.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U00nOGF6n2vk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi4fS2jMqnCC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load training data JSON again\n",
        "with open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json', 'r', encoding='utf-8') as f:\n",
        "    cv_data = json.load(f)\n",
        "\n",
        "# Split into train/dev\n",
        "train_data, dev_data = train_test_split(cv_data, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6QlMtdopRVH"
      },
      "outputs": [],
      "source": [
        "# def convert_to_spacy(data, filename):\n",
        "#     nlp = spacy.blank(\"en\")\n",
        "#     db = DocBin()\n",
        "#     skipped = 0\n",
        "\n",
        "#     for text, annot in tqdm(data):\n",
        "#         doc = nlp.make_doc(text)\n",
        "#         ents = []\n",
        "#         seen_tokens = set()\n",
        "#         valid = True\n",
        "\n",
        "#         for start, end, label in annot[\"entities\"]:\n",
        "#             if start >= end or start < 0 or end > len(text):\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "#             if span is None:\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             if any(tok.i in seen_tokens for tok in span):\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             ents.append(span)\n",
        "#             seen_tokens.update([tok.i for tok in span])\n",
        "\n",
        "#         if valid:\n",
        "#             try:\n",
        "#                 doc.ents = ents\n",
        "#                 db.add(doc)\n",
        "#             except Exception:\n",
        "#                 skipped += 1\n",
        "#         else:\n",
        "#             skipped += 1\n",
        "\n",
        "#     db.to_disk(filename)\n",
        "#     print(f\"✅ Saved {len(data)-skipped} examples to {filename}. Skipped {skipped}.\")\n",
        "\n",
        "\n",
        "def convert_to_spacy(data, filename):\n",
        "    import spacy\n",
        "    from spacy.tokens import DocBin\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    db = DocBin()\n",
        "    skipped = 0\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        text = text.strip()  # clean whitespace\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        seen_tokens = set()\n",
        "        valid = True\n",
        "\n",
        "        for start, end, label in annot[\"entities\"]:\n",
        "            if start >= end or start < 0 or end > len(text):\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span is None or span.text.strip() != span.text:\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            if any(tok.i in seen_tokens for tok in span):\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            ents.append(span)\n",
        "            seen_tokens.update([tok.i for tok in span])\n",
        "\n",
        "        if valid:\n",
        "            try:\n",
        "                doc.ents = ents\n",
        "                db.add(doc)\n",
        "            except Exception:\n",
        "                skipped += 1\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    db.to_disk(filename)\n",
        "    print(f\"✅ Saved {len(data)-skipped} examples to {filename}. Skipped {skipped}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IDicxVGlpaS",
        "outputId": "b079acf5-8146-4a19-b5cc-32d01e2cd2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [00:01<00:00, 118.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 81 examples to train_data.spacy. Skipped 99.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 94.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 8 examples to dev_data.spacy. Skipped 12.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "convert_to_spacy(train_data, \"train_data.spacy\")\n",
        "convert_to_spacy(dev_data, \"dev_data.spacy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD0N0DvINrIi",
        "outputId": "491cb58b-cf66-45a9-a74a-73d36b8075b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(cv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftB0dw1rattX"
      },
      "outputs": [],
      "source": [
        "# you can see the data(json) by uncommenting below statement\n",
        "#cv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E96G8LV4SV8",
        "outputId": "ebc880c8-cae5-4869-d65c-5f746bf547fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "2025-05-22 06:32:26.548078: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-22 06:32:26.566420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747895546.588124   23092 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747895546.594666   23092 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 06:32:26.616180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 5e-05\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0           0.00    240.36    0.00    0.00    0.00    0.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "#!python -m spacy train config.cfg --output ./output --gpu-id 0\n",
        "\n",
        "!python -m spacy train config.cfg --output ./output --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biYUMq1spsd1",
        "outputId": "a75541c6-541d-43e8-c84a-02260d6100fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/CV-Parsing-using-Spacy-3/data/training/config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCv9ydcoYhV_"
      },
      "source": [
        "---\n",
        "config.cfg must be modified in order to reduce the training time<br>\n",
        "(generated in *content/CV-Parsing-using-Spacy-3/data/training/config.cfg*)<br><br>\n",
        "max_steps parameter must be set to some low number like 2000 or 4000, which will drastically reduce the training time.</h3>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OJ7_SGfamRK"
      },
      "source": [
        "This function takes a dataset of text and annotations, processes each text to identify and label entities, and stores the results in a DocBin object that can be used for further training or analysis with spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z54oCPNspsaZ"
      },
      "outputs": [],
      "source": [
        "def get_spacy_doc(file, data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "  for text, annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start, end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity == True:\n",
        "        continue\n",
        "      entity_indices = entity_indices + list(range(start, end))\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if span is None:\n",
        "        err_data = str([start, end]) + ' ' + str(text) + '\\n'\n",
        "        file.write(err_data)\n",
        "      else :\n",
        "        ents.append(span)\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "      #print(text)\n",
        "  return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFpv06L-kcC0"
      },
      "outputs": [],
      "source": [
        "# !pip install optuna\n",
        "# import optuna\n",
        "\n",
        "# def objective(trial):\n",
        "#     dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
        "#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "#     # Update config.cfg\n",
        "#     with open(\"/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\", \"r\") as file:\n",
        "#         config = file.read()\n",
        "\n",
        "#     config = config.replace(\"dropout = 0.2\", f\"dropout = {dropout}\")\n",
        "#     config = config.replace(\"batch_size = 32\", f\"batch_size = {batch_size}\")\n",
        "\n",
        "#     with open(\"/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\", \"w\") as file:\n",
        "#         file.write(config)\n",
        "\n",
        "#     # Train the model\n",
        "#     !python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --gpu-id 0\n",
        "\n",
        "#     # Check if the model was created\n",
        "#     import os\n",
        "#     if os.path.exists(\"./output/model-best\"):  # Check if the model directory exists\n",
        "#         # Load trained model and evaluate\n",
        "#         nlp = spacy.load(\"./output/model-best\")\n",
        "#         test_text = \"John Doe worked at Microsoft as a Data Scientist from 2019-2023.\"\n",
        "#         doc = nlp(test_text)\n",
        "\n",
        "#         return len(doc.ents)  # Optimize based on entity count\n",
        "#     else:\n",
        "#         print(\"Model not found at ./output/model-best. Check training output.\")\n",
        "#         return 0  # Return a default value if the model is not found\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=5)\n",
        "\n",
        "# print(\"Best Hyperparameters:\", study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yp9vtIvkche"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsxMgzn0psYy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBj3E2HpsWE",
        "outputId": "5ff15c58-b090-4231-840a-64e36e26fd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:01<00:00, 103.73it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 78.35it/s]\n"
          ]
        }
      ],
      "source": [
        "file = open('error.txt', 'w')\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXSbjy3IpsTW",
        "outputId": "55f9238e-a43b-41e9-aeff-3b08dac3de58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "2025-05-22 06:35:15.206496: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-22 06:35:15.224643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747895715.246514   23902 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747895715.253145   23902 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 06:35:15.274826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        8506.64   1260.38    0.06    0.04    0.25    0.00\n",
            "  3     200      285877.78  87618.73   23.42   43.05   16.09    0.23\n",
            "  7     400       98950.56  35627.27   43.03   40.39   46.04    0.43\n",
            " 11     600       18730.27  30597.92   55.93   60.20   52.23    0.56\n",
            " 15     800       10635.23  28265.40   56.21   48.43   66.96    0.56\n",
            " 18    1000       13038.13  25627.29   54.59   51.42   58.17    0.55\n",
            " 22    1200        9090.08  25156.46   52.70   47.43   59.28    0.53\n",
            " 26    1400       28460.44  24949.74   58.83   61.15   56.68    0.59\n",
            " 30    1600       19582.59  24980.75   52.88   61.24   46.53    0.53\n",
            " 33    1800        2071.53  23820.88   55.91   47.92   67.08    0.56\n",
            " 37    2000       19156.71  22033.08   57.02   55.36   58.79    0.57\n",
            " 41    2200        3186.07  22607.01   56.38   61.25   52.23    0.56\n",
            " 45    2400         931.92  22589.46   58.80   54.75   63.49    0.59\n",
            " 49    2600       23121.38  22430.31   58.39   51.26   67.82    0.58\n",
            " 52    2800         585.70  20585.19   56.46   57.77   55.20    0.56\n",
            " 56    3000         912.41  20561.01   58.36   58.18   58.54    0.58\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2OcJgxHBr8"
      },
      "source": [
        "<h3>Testing the Trained Model</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzwZDEUjpsPc"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NXhYafFtpsJ9",
        "outputId": "a72896be-a7bb-48f0-a5c8-d5e06df1b6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XksnzmCd5z6y",
        "outputId": "8260e72e-e74a-489d-8d42-d6ee71dba3de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot_MuMut1o4t"
      },
      "outputs": [],
      "source": [
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# text = extract_text(\"/content/M_Abdullah_Resume.pdf\")\n",
        "# print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xkw_Vay2I5F"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "#   print(ent.text, '---->' , ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCGy_UCE3GcX"
      },
      "source": [
        "testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdHAPYuC3AmP"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load your trained model\n",
        "nlp = spacy.load(\"output/model-last\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usY283qg3CLv"
      },
      "outputs": [],
      "source": [
        "# # Read resume text\n",
        "# from pdfminer.high_level import extract_text\n",
        "# text = extract_text(\"/content/M_Abdullah_Resume.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ndwJzt3DjV"
      },
      "outputs": [],
      "source": [
        "# # Process with NER model\n",
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, \"---->\", ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnadhzWA3efl"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# from spacy.matcher import Matcher\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# # Load the trained model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the new resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary to remove duplicates\n",
        "# extracted_data = {\"Name\": set(), \"Degree\": set(), \"College Name\": set(), \"Location\": set(), \"Skills\": set()}\n",
        "\n",
        "# # Extract NER results\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract experience using regex\n",
        "# exp_pattern = re.findall(r'(\\d+)\\+?\\s*years?\\s*of\\s*experience', text, re.IGNORECASE)\n",
        "# if exp_pattern:\n",
        "#     extracted_data[\"Experience\"] = {exp_pattern[0] + \" years\"}\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"---- Cleaned Entity Recognition Output ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {', '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLpaj0LTGsyU"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# from spacy.matcher import Matcher\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# # Load the trained model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the new resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary to remove duplicates\n",
        "# extracted_data = {\"Name\": set(), \"Degree\": set(), \"College Name\": set(), \"Location\": set(), \"Skills\": set()}\n",
        "\n",
        "# # Extract NER results\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract experience using regex\n",
        "# exp_pattern = re.findall(r'(\\d+)\\+?\\s*years?\\s*of\\s*experience', text, re.IGNORECASE)\n",
        "# if exp_pattern:\n",
        "#     extracted_data[\"Experience\"] = {exp_pattern[0] + \" years\"}\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"---- Cleaned Entity Recognition Output ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {', '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cxzDEo5UHsmZ",
        "outputId": "aef6ccc8-a888-49a0-a638-34324b7bc47a"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Designation'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-565fea998967>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Extract Named Entities (NER)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mextracted_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Extract job experience details (using regex)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Designation'"
          ]
        }
      ],
      "source": [
        "# import spacy\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "# from dateutil import parser\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Load the trained NER model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary\n",
        "# extracted_data = {\n",
        "#     \"Name\": set(),\n",
        "#     \"Degree\": set(),\n",
        "#     \"College Name\": set(),\n",
        "#     \"Location\": set(),\n",
        "#     \"Skills\": set(),\n",
        "#     \"Experience\": set(),\n",
        "# }\n",
        "\n",
        "# # Extract Named Entities (NER)\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract job experience details (using regex)\n",
        "# experience_pattern = re.findall(\n",
        "#     r\"([\\w\\s\\-/]+)\\s*[\\n]*\\s*(\\w+\\s*\\d{4})\\s*–\\s*(\\w+\\s*\\d{4}|Present)\", text\n",
        "# )\n",
        "\n",
        "# total_experience = 0\n",
        "# for job_title, start_date, end_date in experience_pattern:\n",
        "#     try:\n",
        "#         start_date = parser.parse(start_date)\n",
        "#         end_date = (\n",
        "#             parser.parse(end_date) if \"Present\" not in end_date else datetime.today()\n",
        "#         )\n",
        "#         experience_months = (end_date.year - start_date.year) * 12 + (\n",
        "#             end_date.month - start_date.month\n",
        "#         )\n",
        "#         total_experience += experience_months\n",
        "#         extracted_data[\"Experience\"].add(\n",
        "#             f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "#         )\n",
        "#     except Exception:\n",
        "#         continue  # Skip parsing errors\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "\n",
        "# # Convert total experience to years & months\n",
        "# if total_experience > 0:\n",
        "#     years, months = divmod(total_experience, 12)\n",
        "#     extracted_data[\"Total Experience\"] = f\"{years} years {months} months\"\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"\\n---- Extracted Information ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {','.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMRfXmc6IX_M",
        "outputId": "bf2734d8-d05d-42dd-c793-1bf6459529a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Extracted Resume Information ----\n",
            "Name:\n",
            "  - Muhammad Fareed\n",
            "  - MUHAMMAD FAREED\n",
            "College Name:\n",
            "  - Hamdard University\n",
            "Skills:\n",
            "  - MERN Stack\n",
            "  - PostgreSQL\n",
            "  - MERN stack\n",
            "  - Python\n",
            "  - FAST API\n",
            "  - React\n",
            "  - Solidity\n",
            "  - JavaScript\n",
            "  - Firebase\n",
            "Degree:\n",
            "  - Web and App Development\n",
            "  - Bachelor in Computer Science\n",
            "Total Experience:\n",
            "  - 3 years 6 months\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Experience + Skills)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the trained NER model\n",
        "nlp = spacy.load(\"output/model-best\")  # Ensure this path is correct\n",
        "\n",
        "# Extract text from the resume PDF\n",
        "text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Store extracted entities dynamically\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    # \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# Extract NER entities\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# Step 1: Extract raw job experience using regex\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "# Step 2: Extract and merge overlapping experience durations\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Step 3: Merge overlapping date ranges\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Step 4: Calculate total merged experience\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# Step 5: Rule-based skill matching\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\",\n",
        "    \"mern stack\", \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# Display extracted results\n",
        "print(\"\\n---- Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(values))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Experience + Skills)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the trained NER model\n",
        "nlp = spacy.load(\"output/model-best\")  # Ensure this path is correct\n",
        "\n",
        "# Extract text from the resume PDF\n",
        "text = extract_text(\"/content/Abdullah_CV (2).pdf\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Store extracted entities dynamically\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    # \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# Extract NER entities\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# Step 1: Extract raw job experience using regex\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "# Step 2: Extract and merge overlapping experience durations\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Step 3: Merge overlapping date ranges\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Step 4: Calculate total merged experience\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# Step 5: Rule-based skill matching\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\",\n",
        "    \"mern stack\", \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# Display extracted results\n",
        "print(\"\\n---- Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(values))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhfOZl7b6Ukh",
        "outputId": "b6b2a79e-d4fe-4f96-81a1-295fb51987ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD ABDULLAH\n",
            "College Name:\n",
            "  - at Hamdard University\n",
            "  - Hamdard University\n",
            "Skills:\n",
            "  - Python\n",
            "  - Data Science\n",
            "  - Machine Learning\n",
            "  - data science\n",
            "Email Address:\n",
            "  - abdullah- 13859617a/\n",
            "  - github.com/M-Abdullah786\n",
            "  - muhammad-\n",
            "  - syed-\n",
            "  - /in/\n",
            "Degree:\n",
            "  - Undergraduate student\n",
            "  - Bachelor of Science - Computer Science\n",
            "-\n",
            "  - Bachelor’s student majoring in Computer Science\n",
            "Total Experience:\n",
            "  - 0 years 3 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuzySYdtLGao",
        "outputId": "2ef52120-1ee0-4175-f735-401bdbb687b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---- Extracted Information ----\n",
            "Name: MUHAMMAD FAREED Muhammad Fareed\n",
            "Degree: Bachelor in Computer Science\n",
            "College Name: Hamdard University Saylani Mass & IT Training Program (SMIT)\n",
            "Web and App Development\n",
            "Location: Karachi S T M E\n",
            "Experience: WORK EXPERIENCE\n",
            "\n",
            "Frontend Developer / Elearning Avenue\n",
            "\n",
            "Oc (Apr 2022 - May 2023)\n",
            "Total Experience: h   t y 1 o m n r s e a\n"
          ]
        }
      ],
      "source": [
        "# import spacy\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "# from dateutil import parser\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Load the trained NER model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary (Use Sets to Avoid Duplicates)\n",
        "# extracted_data = {\n",
        "#     \"Name\": set(),\n",
        "#     \"Degree\": set(),\n",
        "#     \"College Name\": set(),\n",
        "#     \"Location\": set(),\n",
        "#     \"Skills\": set(),\n",
        "#     \"Experience\": set(),\n",
        "# }\n",
        "\n",
        "# # Extract Named Entities (NER)\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text.strip())\n",
        "\n",
        "# # Remove unwanted terms from Skills & Degree\n",
        "# invalid_terms = {\"SKILLS\", \"SUMMARY\", \"WORK EXPERIENCE\", \"EDUCATION\"}\n",
        "# extracted_data[\"Degree\"] -= invalid_terms\n",
        "# extracted_data[\"Skills\"] -= invalid_terms\n",
        "\n",
        "# # Extract skills manually (if missing from NER)\n",
        "# skills_section = re.findall(\n",
        "#     r\"(?i)(?:Skills|Technical Skills|Expertise|SKILLS SUMMARY)[\\s\\S]*?(?=\\n\\n|\\Z)\", text\n",
        "# )\n",
        "# if skills_section:\n",
        "#     skills = re.findall(r\"\\b[A-Za-z#.\\-+]+\\b\", skills_section[0])\n",
        "#     extracted_data[\"Skills\"].update(skill for skill in skills if skill.upper() not in invalid_terms)\n",
        "\n",
        "# # Extract job experience details (using regex)\n",
        "# experience_pattern = re.findall(\n",
        "#     r\"([\\w\\s\\-/]+)\\s*[\\n]*\\s*(\\w+\\s*\\d{4})\\s*–\\s*(\\w+\\s*\\d{4}|Present)\", text\n",
        "# )\n",
        "\n",
        "# total_experience_months = 0\n",
        "# for job_title, start_date, end_date in experience_pattern:\n",
        "#     try:\n",
        "#         start_date = parser.parse(start_date)\n",
        "#         end_date = (\n",
        "#             parser.parse(end_date) if \"Present\" not in end_date else datetime.today()\n",
        "#         )\n",
        "#         experience_months = (end_date.year - start_date.year) * 12 + (\n",
        "#             end_date.month - start_date.month\n",
        "#         )\n",
        "#         total_experience_months += experience_months\n",
        "\n",
        "#         extracted_data[\"Experience\"].add(\n",
        "#             f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "#         )\n",
        "#     except Exception:\n",
        "#         continue  # Skip parsing errors\n",
        "\n",
        "# # Convert total experience to years & months\n",
        "# if total_experience_months > 0:\n",
        "#     years, months = divmod(total_experience_months, 12)\n",
        "#     extracted_data[\"Total Experience\"] = f\"{years} years {months} months\"\n",
        "\n",
        "# # Remove duplicates & format results properly\n",
        "# for key in extracted_data:\n",
        "#     extracted_data[key] = list(set(extracted_data[key]))  # Convert sets to lists\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"\\n---- Extracted Information ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#          print(f\"{key}: {' '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xZi0d1e39ms",
        "outputId": "05572109-7324-4218-adaf-5035c0029d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- Full Resume Text ----\n",
            "MUHAMMAD FAREED\n",
            "M E R N   S T A C K   D E V E L O P E R\n",
            "\n",
            "Muhammad Fareed / LinkedIn\n",
            "mfareed1947 / github\n",
            "\n",
            "Hamdard University\n",
            "Bachelor in Computer Science (7 semester)\n",
            "Saylani Mass & IT Training Program (SMIT)\n",
            "Web and App Development\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "Email: mfareed1504@gmail.com\n",
            "Mobile: +92 3043686779\n",
            "\n",
            "Karachi, Pakistan\n",
            "Nov 2021 – Present\n",
            "Karachi, Pakistan\n",
            "Jan 2021 – 2022\n",
            "\n",
            "• Languages:\n",
            "\n",
            "JavaScript, Python, Typescript , Solidity , Node Js , PostgreSQL\n",
            "\n",
            "• Frameworks:\n",
            "\n",
            "React Js, Next Js, Nuxt Js, Express Js, FAST API, Gatsby Js\n",
            "\n",
            "• Tools:\n",
            "\n",
            "Tailwind CSS, MUI, Shadcn UI, Style component\n",
            "\n",
            "SKILLS SUMMARY\n",
            "\n",
            "• Platforms:\n",
            "\n",
            "MongoDB, Weaviate, Pinecone, Sanity, Supabase, Firebase, Strapi, Stripe, Redux, Drizzle, Neon Js, Shopify\n",
            "\n",
            "• Soft Skills:\n",
            "\n",
            "Rapport Building, Strong Stakeholder Management, People Management, Excellent Communication\n",
            "\n",
            "MERN Stack Developer / Esspfa IT Solution\n",
            "\n",
            "July 2023 –Present\n",
            "\n",
            "◦ Collaborated with cross-functional teams to create intuitive and visually appealing user interfaces, ensuring seamless \n",
            "\n",
            "         integration with backend systems\n",
            "\n",
            "◦ Contributed to the development and maintenance of websites for diverse clients, including GoodGoblin.ai, Torquelist, \n",
            "\n",
            "         BookingTek Secondary DAO, and more, demonstrating adaptability and versatility in handling various project requirements.\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "Frontend Developer / Elearning Avenue\n",
            "\n",
            "Oct 2022 – May 2023\n",
            "\n",
            "◦ Developed and maintained scalable and performant web applications using the Next.js framework and React.js library.\n",
            "\n",
            "Instructor & Trainer / Saylani Mass IT\n",
            "\n",
            "Dec 2023 – Present\n",
            "\n",
            "◦ As an instructor of the MERN stack, I guide students through mastering MongoDB, Express.js, React, and Node.js to\n",
            "\n",
            "        build full-stack web applications.\n",
            "\n",
            "BookingTek / LINK\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "◦ Technologies : Next Js ,Tailwind CSS , Typescript ,React Query \n",
            "◦ BookingTek specializes in providing digital solutions for large hotel and restaurant chains. Their primary product, TableRes, is \n",
            "\n",
            "         a mobile app that facilitates contactless ordering and payments\n",
            "\n",
            "SecondaryDAO / LINK\n",
            "\n",
            "◦ SecondaryDAO enables global investors to enter the real estate market through compliant, fractional,\n",
            "\n",
            "         tokenized ownership powered by blockchain technology.\n",
            "\n",
            "JK-SaaS Chatbot (RAG)\n",
            "\n",
            "◦  Key role in developing the backend for *JK-SaaS*, a chatbot platform utilizing Retrieval-Augmented Generation (RAG) for\n",
            "         SaaS applications. Technologies Used: Python, FastAPI, Weaviate (Vector Database)  Implemented API endpoints for eﬃcient\n",
            "         data retrieval and response generation, ensuring optimal performance and scalability for the chatbot functionalities.\n",
            "\n",
            "Torquelist / LINK\n",
            "\n",
            "◦  TorqueList is an online platform for buying and selling automotive parts and accessories. I developed both\n",
            "\n",
            "          the front end using React.js and the back end with Node.js and Express.js, utilizing MongoDB for data management,\n",
            "          ensuring seamless integration and cloud deployment.\n",
            "\n",
            "Web3 / BLOCKCHAIN PROJECTS\n",
            "\n",
            "◦ Technologies Used: Solidity, Ethereum, Chainlink, AggregatorV3Interface, Smart Contracts, Foundry \n",
            "         Created a smart contract in Solidity on the Foundry framework for a *FundMe* project on the Ethereum\n",
            "         blockchain, with comprehensive testing to ensure secure and transparent functionality.\n",
            "\n",
            "◦ Key role in creating a token marketplace using ERC20 standards in Solidity. Developed smart contracts to facilitate\n",
            "\n",
            "         secure token transactions and marketplace functionalities on the Ethereum blockchain.  Ensured robust security\n",
            "         measures and eﬃcient token management for a seamless user experience.\n",
            "\n",
            "◦ I developed a decentralized voting system ensuring transparency and security with features like voter and candidate\n",
            "\n",
            "         registration, secure voting, voting period management, and result announcement. Future enhancements include\n",
            "         optimizing gas fees and integrating a dApp for a seamless user experience.\n",
            "\n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "# print(\"---- Full Resume Text ----\")\n",
        "# print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qO5hHqnpsIg"
      },
      "outputs": [],
      "source": [
        "# import sys , fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHKQp7vtpsEV"
      },
      "outputs": [],
      "source": [
        "# fname = '/content/CV-Parsing-using-Spacy-3/data/test/Muhammad Raza Resume (1).pdf'\n",
        "# doc = fitz.open(fname)\n",
        "# text = \"\"\n",
        "# for page in doc:\n",
        "  # text = text + str(page.get_text())\n",
        "# doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjIxwf1YpsCy"
      },
      "outputs": [],
      "source": [
        "# text = text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy1-dVU8pr-3"
      },
      "outputs": [],
      "source": [
        "# txt = \" \".join(text.split('\\n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvQw2UishtH0"
      },
      "outputs": [],
      "source": [
        "# print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7d36gGPpr9h"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "  # print(ent.text, '---->' , ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lD2teLSpprzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6967dd-136f-4e8c-8cbf-5054eff202a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/model-last/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-last/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-last/vocab/ (stored 0%)\n",
            "  adding: content/output/model-last/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-last/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-last/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-last/ner/ (stored 0%)\n",
            "  adding: content/output/model-last/ner/model (deflated 8%)\n",
            "  adding: content/output/model-last/ner/moves (deflated 74%)\n",
            "  adding: content/output/model-last/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-last/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-last/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-last/meta.json (deflated 67%)\n",
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-best/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 74%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-best/meta.json (deflated 67%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/output.zip /content/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M2ZJLvBs5ic",
        "outputId": "8185c6f5-2e06-4e56-cf3b-d41c1feabe73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;1m✘ Config validation error\u001b[0m\n",
            "disabled\tfield required\n",
            "tokenizer\tfield required\n",
            "before_creation\tfield required\n",
            "after_creation\tfield required\n",
            "after_pipeline_creation\tfield required\n",
            "batch_size\tfield required\n",
            "{'lang': 'en', 'pipeline': ['transformer', 'ner'], 'vectors': {'@vectors': 'spacy.Vectors.v1'}}\n",
            "\n",
            "If your config contains missing values, you can run the 'init fill-config'\n",
            "command to fill in all the defaults, if possible:\n",
            "\n",
            "python -m spacy init fill-config config.cfg config.cfg \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !python -m spacy debug data config.cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzQ1mwz6YIwP"
      },
      "source": [
        "---\n",
        "<h4>For Reference:</h4>\n",
        "<a href='https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git'>CV-Parsing-using-Spacy-3\n",
        "</a><br>\n",
        "<a href='https://github.com/yashlikescode/spacyResumeParcer.git'>spacyResumeParcer</a>\n",
        "<hr>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}