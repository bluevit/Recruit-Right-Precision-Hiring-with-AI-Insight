{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluevit/Recruit-Right-Precision-Hiring-with-AI-Insight/blob/master/org_NER_for_ResumeAnalysis_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRF5YRktW07P"
      },
      "source": [
        "<h3>Training spaCy NER for Resume Analysis</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOYpCT9v9RBd",
        "outputId": "d13cfcf2-2a58-4dd6-8fa1-19a403435937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy #==1.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv_CRv5cm7Hx",
        "outputId": "d1518b2b-b667-4de5-e898-f662311ecd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.7)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.0.2)\n",
            "Collecting transformers<4.50.0,>=3.4.0 (from spacy-transformers)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
            "  Downloading spacy_alignments-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.33.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy-transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n",
            "Downloading spacy_transformers-1.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (758 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.8/758.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_alignments-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.2/314.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-alignments, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, spacy-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 spacy-alignments-0.9.2 spacy-transformers-1.3.9 transformers-4.49.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy #==3.7.2\n",
        "!pip install spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SgpoPgHMnl1F"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOytCEpK13eM",
        "outputId": "32ae05f1-5ff5-41ca-cffc-d43e14695f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_trf  # Download Transformer-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39pJf9gNnWu3",
        "outputId": "ba82aaba-4875-4b75-d968-9e90f7b4f9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (1.3.9)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.7)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.0.2)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (4.49.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (0.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.33.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy-transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy-transformers\n",
        "import spacy\n",
        "from spacy.tokens import DocBin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPitVvbJ2EV0",
        "outputId": "17899b82-ffff-43e7-86f4-898e8509f331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully written config.cfg and config.json.\n"
          ]
        }
      ],
      "source": [
        "# config_text = \"\"\"[paths]\n",
        "# train = \"./train_data.spacy\"\n",
        "# dev = \"./dev_data.spacy\"\n",
        "# vectors = null\n",
        "\n",
        "# [nlp]\n",
        "# lang = \"en\"\n",
        "# pipeline = [\"transformer\", \"ner\"]\n",
        "\n",
        "# [components]\n",
        "# [components.ner]\n",
        "# factory = \"ner\"\n",
        "\n",
        "# [components.transformer]\n",
        "# factory = \"transformer\"\n",
        "\n",
        "# [training]\n",
        "# dropout = 0.2\n",
        "# patience = 5\n",
        "# max_epochs = 20\n",
        "# seed = 42\n",
        "# gpu_allocator = \"pytorch\"\n",
        "# accumulate_gradient = 1\n",
        "# max_steps = 20000\n",
        "# eval_frequency = 200\n",
        "# frozen_components = []\n",
        "# annotating_components = []\n",
        "# dev_corpus = \"corpora.dev\"\n",
        "# train_corpus = \"corpora.train\"\n",
        "# before_to_disk = null\n",
        "# before_update = null\n",
        "\n",
        "# [training.logger]\n",
        "# @loggers = \"spacy.ConsoleLogger.v1\"\n",
        "\n",
        "# [training.batcher]\n",
        "# @batchers = \"spacy.batch_by_words.v1\"\n",
        "# discard_oversize = false\n",
        "# tolerance = 0.2\n",
        "# [training.batcher.size]\n",
        "# @schedules = \"compounding.v1\"\n",
        "# start = 100\n",
        "# stop = 1000\n",
        "# compound = 1.001\n",
        "\n",
        "# [training.optimizer]\n",
        "# @optimizers = \"Adam.v1\"\n",
        "# learn_rate = 0.00005\n",
        "\n",
        "# [training.score_weights]\n",
        "# \"\"\"\n",
        "\n",
        "# with open(\"config.cfg\", \"w\") as f:\n",
        "#     f.write(config_text)\n",
        "\n",
        "# print(\"✅ Final corrected config.cfg written.\")\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# --- config.cfg content ---\n",
        "config_text = \"\"\"[paths]\n",
        "train = \"./train_data.spacy\"\n",
        "dev = \"./dev_data.spacy\"\n",
        "vectors = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"transformer\", \"ner\"]\n",
        "\n",
        "[components]\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.transformer]\n",
        "factory = \"transformer\"\n",
        "\n",
        "[training]\n",
        "dropout = 0.2\n",
        "patience = 5\n",
        "max_epochs = 20\n",
        "seed = 42\n",
        "gpu_allocator = \"pytorch\"\n",
        "accumulate_gradient = 1\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "annotating_components = []\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "before_to_disk = null\n",
        "before_update = null\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "learn_rate = 0.00005\n",
        "\n",
        "[training.score_weights]\n",
        "\"\"\"\n",
        "\n",
        "# --- JSON version of the same config ---\n",
        "config_json = {\n",
        "    \"paths\": {\n",
        "        \"train\": \"./train_data.spacy\",\n",
        "        \"dev\": \"./dev_data.spacy\",\n",
        "        \"vectors\": None\n",
        "    },\n",
        "    \"nlp\": {\n",
        "        \"lang\": \"en\",\n",
        "        \"pipeline\": [\"transformer\", \"ner\"]\n",
        "    },\n",
        "    \"components\": {\n",
        "        \"ner\": {\"factory\": \"ner\"},\n",
        "        \"transformer\": {\"factory\": \"transformer\"}\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"dropout\": 0.2,\n",
        "        \"patience\": 5,\n",
        "        \"max_epochs\": 20,\n",
        "        \"seed\": 42,\n",
        "        \"gpu_allocator\": \"pytorch\",\n",
        "        \"accumulate_gradient\": 1,\n",
        "        \"max_steps\": 20000,\n",
        "        \"eval_frequency\": 200,\n",
        "        \"frozen_components\": [],\n",
        "        \"annotating_components\": [],\n",
        "        \"dev_corpus\": \"corpora.dev\",\n",
        "        \"train_corpus\": \"corpora.train\",\n",
        "        \"before_to_disk\": None,\n",
        "        \"before_update\": None,\n",
        "        \"logger\": {\n",
        "            \"@loggers\": \"spacy.ConsoleLogger.v1\"\n",
        "        },\n",
        "        \"batcher\": {\n",
        "            \"@batchers\": \"spacy.batch_by_words.v1\",\n",
        "            \"discard_oversize\": False,\n",
        "            \"tolerance\": 0.2,\n",
        "            \"size\": {\n",
        "                \"@schedules\": \"compounding.v1\",\n",
        "                \"start\": 100,\n",
        "                \"stop\": 1000,\n",
        "                \"compound\": 1.001\n",
        "            }\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"@optimizers\": \"Adam.v1\",\n",
        "            \"learn_rate\": 0.00005\n",
        "        },\n",
        "        \"score_weights\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Write files ---\n",
        "with open(\"config.cfg\", \"w\") as cfg_file:\n",
        "    cfg_file.write(config_text)\n",
        "\n",
        "with open(\"config.json\", \"w\") as json_file:\n",
        "    json.dump(config_json, json_file, indent=2)\n",
        "\n",
        "print(\"✅ Successfully written config.cfg and config.json.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ap7YCy2E2HEG"
      },
      "outputs": [],
      "source": [
        "# !cat config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glaUB9zsolEf",
        "outputId": "f8345bec-ca21-4268-8596-b3291ee93ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-Parsing-using-Spacy-3'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 82 (delta 14), reused 74 (delta 14), pack-reused 6 (from 1)\u001b[K\n",
            "Receiving objects: 100% (82/82), 5.62 MiB | 10.69 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git\n",
        "\n",
        "# !git clone https://github.com/Mehyarmlaweh/NER-Annotated-CVs.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U00nOGF6n2vk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pi4fS2jMqnCC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load training data JSON again\n",
        "with open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json', 'r', encoding='utf-8') as f:\n",
        "    cv_data = json.load(f)\n",
        "\n",
        "# Split into train/dev\n",
        "train_data, dev_data = train_test_split(cv_data, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I6QlMtdopRVH"
      },
      "outputs": [],
      "source": [
        "# # def convert_to_spacy(data, filename):\n",
        "# #     nlp = spacy.blank(\"en\")\n",
        "# #     db = DocBin()\n",
        "# #     skipped = 0\n",
        "\n",
        "# #     for text, annot in tqdm(data):\n",
        "# #         doc = nlp.make_doc(text)\n",
        "# #         ents = []\n",
        "# #         seen_tokens = set()\n",
        "# #         valid = True\n",
        "\n",
        "# #         for start, end, label in annot[\"entities\"]:\n",
        "# #             if start >= end or start < 0 or end > len(text):\n",
        "# #                 valid = False\n",
        "# #                 break\n",
        "\n",
        "# #             span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "# #             if span is None:\n",
        "# #                 valid = False\n",
        "# #                 break\n",
        "\n",
        "# #             if any(tok.i in seen_tokens for tok in span):\n",
        "# #                 valid = False\n",
        "# #                 break\n",
        "\n",
        "# #             ents.append(span)\n",
        "# #             seen_tokens.update([tok.i for tok in span])\n",
        "\n",
        "# #         if valid:\n",
        "# #             try:\n",
        "# #                 doc.ents = ents\n",
        "# #                 db.add(doc)\n",
        "# #             except Exception:\n",
        "# #                 skipped += 1\n",
        "# #         else:\n",
        "# #             skipped += 1\n",
        "\n",
        "# #     db.to_disk(filename)\n",
        "# #     print(f\"✅ Saved {len(data)-skipped} examples to {filename}. Skipped {skipped}.\")\n",
        "\n",
        "\n",
        "# def convert_to_spacy(data, filename):\n",
        "#     import spacy\n",
        "#     from spacy.tokens import DocBin\n",
        "#     from tqdm import tqdm\n",
        "\n",
        "#     nlp = spacy.blank(\"en\")\n",
        "#     db = DocBin()\n",
        "#     skipped = 0\n",
        "\n",
        "#     for text, annot in tqdm(data):\n",
        "#         text = text.strip()  # clean whitespace\n",
        "#         doc = nlp.make_doc(text)\n",
        "#         ents = []\n",
        "#         seen_tokens = set()\n",
        "#         valid = True\n",
        "\n",
        "#         for start, end, label in annot[\"entities\"]:\n",
        "#             if start >= end or start < 0 or end > len(text):\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "#             if span is None or span.text.strip() != span.text:\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             if any(tok.i in seen_tokens for tok in span):\n",
        "#                 valid = False\n",
        "#                 break\n",
        "\n",
        "#             ents.append(span)\n",
        "#             seen_tokens.update([tok.i for tok in span])\n",
        "\n",
        "#         if valid:\n",
        "#             try:\n",
        "#                 doc.ents = ents\n",
        "#                 db.add(doc)\n",
        "#             except Exception:\n",
        "#                 skipped += 1\n",
        "#         else:\n",
        "#             skipped += 1\n",
        "\n",
        "#     db.to_disk(filename)\n",
        "#     print(f\"✅ Saved {len(data)-skipped} examples to {filename}. Skipped {skipped}.\")\n",
        "\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "def convert_to_spacy_and_extract(data, filename, model_path):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    db = DocBin()\n",
        "    skipped = 0\n",
        "\n",
        "    print(\"📦 Converting training data to .spacy format...\")\n",
        "    for text, annot in tqdm(data):\n",
        "        text = text.strip()  # Clean whitespace\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        seen_tokens = set()\n",
        "        valid = True\n",
        "\n",
        "        for start, end, label in annot[\"entities\"]:\n",
        "            if start >= end or start < 0 or end > len(text):\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span is None or span.text.strip() != span.text:\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            if any(tok.i in seen_tokens for tok in span):\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "            ents.append(span)\n",
        "            seen_tokens.update([tok.i for tok in span])\n",
        "\n",
        "        if valid:\n",
        "            try:\n",
        "                doc.ents = ents\n",
        "                db.add(doc)\n",
        "            except Exception:\n",
        "                skipped += 1\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    db.to_disk(filename)\n",
        "    print(f\"✅ Saved {len(data) - skipped} examples to {filename}. Skipped {skipped}.\")\n",
        "\n",
        "    # Now load the trained model and extract entities from each sample\n",
        "    if not model_path:\n",
        "        raise ValueError(\"❌ Please provide the path to your trained NER model.\")\n",
        "\n",
        "    print(\"\\n🔍 Extracting entities using trained model...\\n\")\n",
        "    ner_model = spacy.load(model_path)\n",
        "\n",
        "    for text, _ in data:\n",
        "        print(f\"📝 Text: {text.strip()}\")\n",
        "        doc = ner_model(text)\n",
        "        for ent in doc.ents:\n",
        "            print(f\"{ent.text:<20} → {ent.label_}\")\n",
        "        print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IDicxVGlpaS",
        "outputId": "937565b4-c4de-4e40-e4b6-c6639f7820d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [00:01<00:00, 148.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 81 examples to train_data.spacy. Skipped 99.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 118.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 8 examples to dev_data.spacy. Skipped 12.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "convert_to_spacy(train_data, \"train_data.spacy\")\n",
        "convert_to_spacy(dev_data, \"dev_data.spacy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD0N0DvINrIi"
      },
      "outputs": [],
      "source": [
        "# len(cv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftB0dw1rattX"
      },
      "outputs": [],
      "source": [
        "# you can see the data(json) by uncommenting below statement\n",
        "#cv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E96G8LV4SV8",
        "outputId": "d13f8f0c-6f48-4300-cbcd-9782f9a0195c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 174kB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 3.21MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 25.7MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 3.12MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.90MB/s]\n",
            "2025-07-12 10:51:35.634135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752317495.882519    3336 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752317495.952206    3336 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-12 10:51:36.482315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 499M/499M [00:06<00:00, 75.0MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 5e-05\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0           0.00    240.36    0.00    0.00    0.00    0.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "#!python -m spacy train config.cfg --output ./output --gpu-id 0\n",
        "\n",
        "!python -m spacy train config.cfg --output ./output --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biYUMq1spsd1",
        "outputId": "678d1a2b-636c-4dee-e691-5245d196b30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/CV-Parsing-using-Spacy-3/data/training/config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCv9ydcoYhV_"
      },
      "source": [
        "---\n",
        "config.cfg must be modified in order to reduce the training time<br>\n",
        "(generated in *content/CV-Parsing-using-Spacy-3/data/training/config.cfg*)<br><br>\n",
        "max_steps parameter must be set to some low number like 2000 or 4000, which will drastically reduce the training time.</h3>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OJ7_SGfamRK"
      },
      "source": [
        "This function takes a dataset of text and annotations, processes each text to identify and label entities, and stores the results in a DocBin object that can be used for further training or analysis with spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z54oCPNspsaZ"
      },
      "outputs": [],
      "source": [
        "# def get_spacy_doc(file, data):\n",
        "#   nlp = spacy.blank('en')\n",
        "#   db = DocBin()\n",
        "#   for text, annot in tqdm(data):\n",
        "#     doc = nlp.make_doc(text)\n",
        "#     annot = annot['entities']\n",
        "\n",
        "#     ents = []\n",
        "#     entity_indices = []\n",
        "\n",
        "#     for start, end, label in annot:\n",
        "#       skip_entity = False\n",
        "#       for idx in range(start, end):\n",
        "#         if idx in entity_indices:\n",
        "#           skip_entity = True\n",
        "#           break\n",
        "#       if skip_entity == True:\n",
        "#         continue\n",
        "#       entity_indices = entity_indices + list(range(start, end))\n",
        "#       try:\n",
        "#         span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "#       except:\n",
        "#         continue\n",
        "\n",
        "#       if span is None:\n",
        "#         err_data = str([start, end]) + ' ' + str(text) + '\\n'\n",
        "#         file.write(err_data)\n",
        "#       else :\n",
        "#         ents.append(span)\n",
        "#     try:\n",
        "#       doc.ents = ents\n",
        "#       db.add(doc)\n",
        "#     except:\n",
        "#       pass\n",
        "#       #print(text)\n",
        "#   return db\n",
        "\n",
        "def get_spacy_doc(file, data):\n",
        "    import spacy\n",
        "    from spacy.tokens import DocBin\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    nlp = spacy.blank('en')\n",
        "    db = DocBin()\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        text = text.strip()\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        entity_indices = set()\n",
        "\n",
        "        for start, end, label in annot['entities']:\n",
        "            # Skip overlapping spans\n",
        "            if any(i in entity_indices for i in range(start, end)):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if span is None:\n",
        "                file.write(f\"Invalid span: [{start}, {end}], label: {label}, text: '{text[start:end]}'\\n\")\n",
        "                continue\n",
        "\n",
        "            ents.append(span)\n",
        "            entity_indices.update(range(start, end))\n",
        "\n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFpv06L-kcC0"
      },
      "outputs": [],
      "source": [
        "# !pip install optuna\n",
        "# import optuna\n",
        "\n",
        "# def objective(trial):\n",
        "#     dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
        "#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "#     # Update config.cfg\n",
        "#     with open(\"/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\", \"r\") as file:\n",
        "#         config = file.read()\n",
        "\n",
        "#     config = config.replace(\"dropout = 0.2\", f\"dropout = {dropout}\")\n",
        "#     config = config.replace(\"batch_size = 32\", f\"batch_size = {batch_size}\")\n",
        "\n",
        "#     with open(\"/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\", \"w\") as file:\n",
        "#         file.write(config)\n",
        "\n",
        "#     # Train the model\n",
        "#     !python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --gpu-id 0\n",
        "\n",
        "#     # Check if the model was created\n",
        "#     import os\n",
        "#     if os.path.exists(\"./output/model-best\"):  # Check if the model directory exists\n",
        "#         # Load trained model and evaluate\n",
        "#         nlp = spacy.load(\"./output/model-best\")\n",
        "#         test_text = \"John Doe worked at Microsoft as a Data Scientist from 2019-2023.\"\n",
        "#         doc = nlp(test_text)\n",
        "\n",
        "#         return len(doc.ents)  # Optimize based on entity count\n",
        "#     else:\n",
        "#         print(\"Model not found at ./output/model-best. Check training output.\")\n",
        "#         return 0  # Return a default value if the model is not found\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=5)\n",
        "\n",
        "# print(\"Best Hyperparameters:\", study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yp9vtIvkche"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jsxMgzn0psYy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBj3E2HpsWE",
        "outputId": "00464eb2-2942-4efb-cc54-87865194c25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:01<00:00, 128.80it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 123.93it/s]\n"
          ]
        }
      ],
      "source": [
        "file = open('error.txt', 'w')\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXSbjy3IpsTW",
        "outputId": "62fed73c-1dff-47af-d4c9-2c7359a5288f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "2025-07-12 10:54:50.034463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752317690.054402    4198 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752317690.060322    4198 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-12 10:54:50.082818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        1553.85   1622.78    0.10    0.05    2.01    0.00\n",
            "  4     200      170742.83  66946.12   26.33   58.30   17.00    0.26\n",
            "  8     400       87787.19  25494.02   47.88   66.81   37.31    0.48\n",
            " 12     600        8615.08  20621.21   53.45   66.09   44.86    0.53\n",
            " 16     800       32570.21  19206.10   56.69   53.94   59.74    0.57\n",
            " 20    1000        5321.59  17365.53   56.18   53.78   58.80    0.56\n",
            " 25    1200        1802.72  16027.29   57.18   58.45   55.96    0.57\n",
            " 29    1400        6607.37  14570.91   57.34   67.09   50.06    0.57\n",
            " 33    1600        3749.26  13961.88   55.44   68.34   46.64    0.55\n",
            " 37    1800         633.33  13416.07   54.63   64.57   47.34    0.55\n",
            " 41    2000       10449.69  13264.16   59.52   70.55   51.48    0.60\n",
            " 45    2200         388.20  12584.38   58.20   69.04   50.30    0.58\n",
            " 50    2400        4110.78  12499.68   60.64   61.56   59.74    0.61\n",
            " 54    2600         255.31  11052.96   60.05   57.81   62.46    0.60\n",
            " 58    2800         761.57  10470.54   61.28   60.37   62.22    0.61\n",
            " 62    3000         337.52   9581.30   60.21   67.00   54.66    0.60\n",
            " 66    3200         943.22   8826.07   59.10   63.84   55.02    0.59\n",
            " 70    3400         263.33   7746.83   61.17   64.90   57.85    0.61\n",
            " 75    3600         206.10   6773.14   60.10   66.57   54.78    0.60\n",
            " 79    3800        8370.12   5465.04   60.33   64.64   56.55    0.60\n",
            " 83    4000         338.67   4173.82   57.90   68.44   50.18    0.58\n",
            " 87    4200       13310.64   3309.93   61.41   68.56   55.61    0.61\n",
            " 91    4400         165.71   2224.86   60.32   66.76   55.02    0.60\n",
            " 95    4600         414.47   1531.91   58.76   62.73   55.25    0.59\n",
            "100    4800         289.70   1058.39   62.99   67.07   59.39    0.63\n",
            "104    5000         262.56    647.75   59.17   62.05   56.55    0.59\n",
            "108    5200         534.98    445.22   61.30   65.63   57.50    0.61\n",
            "112    5400         251.35    297.50   62.04   63.99   60.21    0.62\n",
            "116    5600         200.42    204.87   63.17   67.93   59.03    0.63\n",
            "120    5800         170.79    131.56   61.13   70.70   53.84    0.61\n",
            "125    6000         308.23    132.47   63.55   67.42   60.09    0.64\n",
            "129    6200         630.66    111.26   61.55   65.90   57.73    0.62\n",
            "133    6400         823.58    127.41   60.05   66.62   54.66    0.60\n",
            "137    6600         235.40     89.50   62.33   69.42   56.55    0.62\n",
            "141    6800         395.35    125.74   63.03   65.56   60.68    0.63\n",
            "145    7000         231.25     95.54   60.24   64.59   56.43    0.60\n",
            "150    7200         298.90     95.53   61.90   66.09   58.21    0.62\n",
            "154    7400         213.91     69.31   62.73   63.37   62.10    0.63\n",
            "158    7600         550.74     81.94   63.19   65.77   60.80    0.63\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2OcJgxHBr8"
      },
      "source": [
        "<h3>Testing the Trained Model</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hzwZDEUjpsPc"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NXhYafFtpsJ9",
        "outputId": "1224fdda-f88d-4bb7-a4da-0bc7f0218abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XksnzmCd5z6y",
        "outputId": "488f29f0-116e-45b3-e933-1cedd5229a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ot_MuMut1o4t"
      },
      "outputs": [],
      "source": [
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# text = extract_text(\"/content/M_Abdullah_Resume.pdf\")\n",
        "# print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xkw_Vay2I5F"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "#   print(ent.text, '---->' , ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCGy_UCE3GcX"
      },
      "source": [
        "testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WdHAPYuC3AmP"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load your trained model\n",
        "nlp = spacy.load(\"output/model-last\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usY283qg3CLv"
      },
      "outputs": [],
      "source": [
        "# # Read resume text\n",
        "# from pdfminer.high_level import extract_text\n",
        "# text = extract_text(\"/content/M_Abdullah_Resume.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ndwJzt3DjV"
      },
      "outputs": [],
      "source": [
        "# # Process with NER model\n",
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, \"---->\", ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnadhzWA3efl"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# from spacy.matcher import Matcher\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# # Load the trained model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the new resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary to remove duplicates\n",
        "# extracted_data = {\"Name\": set(), \"Degree\": set(), \"College Name\": set(), \"Location\": set(), \"Skills\": set()}\n",
        "\n",
        "# # Extract NER results\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract experience using regex\n",
        "# exp_pattern = re.findall(r'(\\d+)\\+?\\s*years?\\s*of\\s*experience', text, re.IGNORECASE)\n",
        "# if exp_pattern:\n",
        "#     extracted_data[\"Experience\"] = {exp_pattern[0] + \" years\"}\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"---- Cleaned Entity Recognition Output ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {', '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLpaj0LTGsyU"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# from spacy.matcher import Matcher\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "\n",
        "# # Load the trained model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the new resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary to remove duplicates\n",
        "# extracted_data = {\"Name\": set(), \"Degree\": set(), \"College Name\": set(), \"Location\": set(), \"Skills\": set()}\n",
        "\n",
        "# # Extract NER results\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract experience using regex\n",
        "# exp_pattern = re.findall(r'(\\d+)\\+?\\s*years?\\s*of\\s*experience', text, re.IGNORECASE)\n",
        "# if exp_pattern:\n",
        "#     extracted_data[\"Experience\"] = {exp_pattern[0] + \" years\"}\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"---- Cleaned Entity Recognition Output ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {', '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzDEo5UHsmZ"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "# from dateutil import parser\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Load the trained NER model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary\n",
        "# extracted_data = {\n",
        "#     \"Name\": set(),\n",
        "#     \"Degree\": set(),\n",
        "#     \"College Name\": set(),\n",
        "#     \"Location\": set(),\n",
        "#     \"Skills\": set(),\n",
        "#     \"Experience\": set(),\n",
        "# }\n",
        "\n",
        "# # Extract Named Entities (NER)\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text)\n",
        "\n",
        "# # Extract job experience details (using regex)\n",
        "# experience_pattern = re.findall(\n",
        "#     r\"([\\w\\s\\-/]+)\\s*[\\n]*\\s*(\\w+\\s*\\d{4})\\s*–\\s*(\\w+\\s*\\d{4}|Present)\", text\n",
        "# )\n",
        "\n",
        "# total_experience = 0\n",
        "# for job_title, start_date, end_date in experience_pattern:\n",
        "#     try:\n",
        "#         start_date = parser.parse(start_date)\n",
        "#         end_date = (\n",
        "#             parser.parse(end_date) if \"Present\" not in end_date else datetime.today()\n",
        "#         )\n",
        "#         experience_months = (end_date.year - start_date.year) * 12 + (\n",
        "#             end_date.month - start_date.month\n",
        "#         )\n",
        "#         total_experience += experience_months\n",
        "#         extracted_data[\"Experience\"].add(\n",
        "#             f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "#         )\n",
        "#     except Exception:\n",
        "#         continue  # Skip parsing errors\n",
        "\n",
        "# # Rule-based matching for skills\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "# skills_patterns = [\n",
        "#     [{\"LOWER\": \"python\"}],\n",
        "#     [{\"LOWER\": \"javascript\"}],\n",
        "#     [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "#     [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "#     [{\"LOWER\": \"mern\"}, {\"LOWER\": \"stack\"}],\n",
        "#     [{\"LOWER\": \"react\"}],\n",
        "#     [{\"LOWER\": \"firebase\"}],\n",
        "#     [{\"LOWER\": \"fast\"}, {\"LOWER\": \"api\"}],\n",
        "#     [{\"LOWER\": \"solidity\"}],\n",
        "#     [{\"LOWER\": \"postgresql\"}]\n",
        "# ]\n",
        "# matcher.add(\"SKILLS\", skills_patterns)\n",
        "\n",
        "# matches = matcher(doc)\n",
        "# for match_id, start, end in matches:\n",
        "#     extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "\n",
        "# # Convert total experience to years & months\n",
        "# if total_experience > 0:\n",
        "#     years, months = divmod(total_experience, 12)\n",
        "#     extracted_data[\"Total Experience\"] = f\"{years} years {months} months\"\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"\\n---- Extracted Information ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#         print(f\"{key}: {','.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMRfXmc6IX_M",
        "outputId": "3ff41075-fab3-4dc2-f0cc-b462cd7901e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD ABDULLAH\n",
            "College Name:\n",
            "  - Hamdard University\n",
            "  - Government Degree College SRE-Majeed, Karachi\n",
            "\n",
            "2017/April\n",
            "2019/July\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "Matriculation - Science\n",
            "-\n",
            "Computer Science\n",
            "\n",
            "Happy Palace Grammar School, Karachi\n",
            "\n",
            "Machine Learning Object Detection\n",
            "\n",
            "GitHub Link\n",
            "The model is a real-time image classification system designed to predict the class of objects (e.g., fruits\n",
            "and vegetables) captured through a live video feed from a webcam. It uses a pre-trained deep learning\n",
            "model loaded from a .h5 file to classify images into predefined classes.\n",
            "\n",
            "Power BI\n",
            "\n",
            "HR Dashboard\n",
            "A basic analysis of the employees of the company.\n",
            "\n",
            "Machine Learning Stock Prediction\n",
            "\n",
            "A basic prediction of stocks, using Linear Regression.\n",
            "\n",
            "EXPERIENCE\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "Feb 2025 – Present Project Manager Intern\n",
            "\n",
            "Binate Digital\n",
            "Assisting in managing software development projects, coordinating with cross-functional teams, and en-\n",
            "suring timely delivery of project milestones.\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "February/2024\n",
            "\n",
            "Data Analyst in Power BI Certificate\n",
            "\n",
            "September/2023\n",
            "\n",
            "IBM Data Science Certificate\n",
            "\n",
            "ACTIVITIES\n",
            "\n",
            "2022-2023\n",
            "\n",
            "Member of GDSC\n",
            "Was a member.\n",
            "\n",
            "DataCamp\n",
            "\n",
            "Coursera\n",
            "\n",
            "FEST, Hamdard University\n",
            "Location:\n",
            "  - Hamdard\n",
            "  - Karachi\n",
            "Skills:\n",
            "  - data science\n",
            "  - Data Science\n",
            "  - Python\n",
            "  - Machine Learning\n",
            "Degree:\n",
            "  - Bachelor’s student majoring in Computer Science\n",
            "Total Experience:\n",
            "  - 0 years 5 months\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Experience + Skills)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the trained NER model\n",
        "nlp = spacy.load(\"output/model-best\")  # Ensure this path is correct\n",
        "\n",
        "# Extract text from the resume PDF\n",
        "text = extract_text(\"/content/Abdullah_CV.pdf\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Store extracted entities dynamically\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    # \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# Extract NER entities\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# Step 1: Extract raw job experience using regex\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "# Step 2: Extract and merge overlapping experience durations\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Step 3: Merge overlapping date ranges\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Step 4: Calculate total merged experience\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# Step 5: Rule-based skill matching\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\",\n",
        "    \"mern stack\", \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# Display extracted results\n",
        "print(\"\\n---- Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(values))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Experience + Skills)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the trained NER model\n",
        "nlp = spacy.load(\"output/model-best\")  # Ensure this path is correct\n",
        "\n",
        "# Extract text from the resume PDF\n",
        "text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Store extracted entities dynamically\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    # \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    # \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# Extract NER entities\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# Step 1: Extract raw job experience using regex\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "# Step 2: Extract and merge overlapping experience durations\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Step 3: Merge overlapping date ranges\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Step 4: Calculate total merged experience\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# Step 5: Rule-based skill matching\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\",\n",
        "    \"mern stack\", \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# Display extracted results\n",
        "print(\"\\n---- Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(values))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhfOZl7b6Ukh",
        "outputId": "a5f07130-b44b-401e-e770-6fad94dcdbbe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD FAREED\n",
            "College Name:\n",
            "  - Hamdard University\n",
            "Skills:\n",
            "  - MERN stack\n",
            "  - FAST API\n",
            "  - Firebase\n",
            "  - MERN Stack\n",
            "  - React\n",
            "  - PostgreSQL\n",
            "  - Python\n",
            "  - Solidity\n",
            "  - JavaScript\n",
            "Location:\n",
            "  - Muhammad\n",
            "Total Experience:\n",
            "  - 3 years 8 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Email + Experience + Skills + Last Education)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Load the trained spaCy NER model\n",
        "# -----------------------------\n",
        "nlp = spacy.load(\"output/model-best\")  # Update this path as needed\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Extract text from PDF resume\n",
        "# -----------------------------\n",
        "resume_path = \"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\"\n",
        "text = extract_text(resume_path)\n",
        "doc = nlp(text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Initialize data container\n",
        "# -----------------------------\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    # \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    \"Email\": set(),\n",
        "    \"Last Education\": set(),\n",
        "    \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 NER-Based Entity Extraction\n",
        "# -----------------------------\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Email Extraction (with constraints)\n",
        "# -----------------------------\n",
        "email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "emails = re.findall(email_pattern, text)\n",
        "for email in emails:\n",
        "    if len(email) > 5 and \".\" in email.split(\"@\")[-1]:\n",
        "        extracted_data[\"Email\"].add(email.lower())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Experience Extraction from date patterns\n",
        "# -----------------------------\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Merge overlapping experience durations\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Calculate total experience duration\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Rule-Based Skill Matching\n",
        "# -----------------------------\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\", \"mern stack\",\n",
        "    \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Last Education Detection (latest mentioned college/university)\n",
        "# -----------------------------\n",
        "education_labels = [\"College Name\", \"University\", \"Institute\", \"School\", \"Education\"]\n",
        "education_entities = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in education_labels:\n",
        "        education_entities.append((ent.start_char, ent.text.strip()))\n",
        "\n",
        "if education_entities:\n",
        "    education_entities.sort()  # sort by text position\n",
        "    last_education = education_entities[-1][1]\n",
        "    extracted_data[\"Last Education\"] = {last_education}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Display Extracted Results\n",
        "# -----------------------------\n",
        "print(\"\\n---- 📄 Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(sorted(values)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrpoeGwkzxgx",
        "outputId": "4823e29a-73ca-40a1-8a51-dcb9195a0b55"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- 📄 Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD FAREED\n",
            "College Name:\n",
            "  - Hamdard University\n",
            "Skills:\n",
            "  - FAST API\n",
            "  - Firebase\n",
            "  - JavaScript\n",
            "  - MERN Stack\n",
            "  - MERN stack\n",
            "  - PostgreSQL\n",
            "  - Python\n",
            "  - React\n",
            "  - Solidity\n",
            "Email:\n",
            "  - mfareed1504@gmail.com\n",
            "Last Education:\n",
            "  - Hamdard University\n",
            "Experience:\n",
            "  - Frontend Developer / Elearning Avenue (Oct 2022 - May 2023)\n",
            "  - Instructor & Trainer / Saylani Mass IT (Dec 2023 - Jul 2025)\n",
            "  - Karachi, Pakistan (Nov 2021 - Jul 2025)\n",
            "  - MERN Stack Developer / Esspfa IT Solution (Jul 2023 - Jul 2025)\n",
            "Location:\n",
            "  - Muhammad\n",
            "Total Experience:\n",
            "  - 3 years 8 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Email + Experience + Skills + Last Education)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Load the trained spaCy NER model\n",
        "# -----------------------------\n",
        "nlp = spacy.load(\"output/model-best\")  # Update this path as needed\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Extract text from PDF resume\n",
        "# -----------------------------\n",
        "resume_path = \"/content/Muhammad Naeemuddin Resume.pdf\"\n",
        "text = extract_text(resume_path)\n",
        "doc = nlp(text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Initialize data container\n",
        "# -----------------------------\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    # \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    \"Email\": set(),\n",
        "    \"Last Education\": set(),\n",
        "    \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 NER-Based Entity Extraction\n",
        "# -----------------------------\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Email Extraction (with constraints)\n",
        "# -----------------------------\n",
        "email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "emails = re.findall(email_pattern, text)\n",
        "for email in emails:\n",
        "    if len(email) > 5 and \".\" in email.split(\"@\")[-1]:\n",
        "        extracted_data[\"Email\"].add(email.lower())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Experience Extraction from date patterns\n",
        "# -----------------------------\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Merge overlapping experience durations\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Calculate total experience duration\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Rule-Based Skill Matching\n",
        "# -----------------------------\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\", \"mern stack\",\n",
        "    \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Last Education Detection (latest mentioned college/university)\n",
        "# -----------------------------\n",
        "education_labels = [\"College Name\", \"University\", \"Institute\", \"School\", \"Education\"]\n",
        "education_entities = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in education_labels:\n",
        "        education_entities.append((ent.start_char, ent.text.strip()))\n",
        "\n",
        "if education_entities:\n",
        "    education_entities.sort()  # sort by text position\n",
        "    last_education = education_entities[-1][1]\n",
        "    extracted_data[\"Last Education\"] = {last_education}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Display Extracted Results\n",
        "# -----------------------------\n",
        "print(\"\\n---- 📄 Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(sorted(values)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_CKcMBh0qyz",
        "outputId": "ee5de8af-6cef-4d39-b997-66ba4510c198"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- 📄 Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD NAEEMUDDIN\n",
            "College Name:\n",
            "  - St. Patrick’s High School\n",
            "Skills:\n",
            "  - Firebase\n",
            "  - JavaScript\n",
            "  - Javascript\n",
            "  - PostgreSQL\n",
            "  - Python\n",
            "Email:\n",
            "  - naeemuddin24@gmail.com\n",
            "Last Education:\n",
            "  - St. Patrick’s High School\n",
            "Location:\n",
            "  - Karachi\n",
            "Email Address:\n",
            "  - https://github.com/MuhammadNaeemuddin\n",
            "Total Experience:\n",
            "  - 0 years 0 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Email + Experience + Skills + Last Education)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Load the trained spaCy NER model\n",
        "# -----------------------------\n",
        "nlp = spacy.load(\"output/model-best\")  # Update this path as needed\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Extract text from PDF resume\n",
        "# -----------------------------\n",
        "resume_path = \"/content/Muhammad Raza Resume (1).pdf\"\n",
        "text = extract_text(resume_path)\n",
        "doc = nlp(text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Initialize data container\n",
        "# -----------------------------\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    # \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    \"Email\": set(),\n",
        "    \"Last Education\": set(),\n",
        "    \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 NER-Based Entity Extraction\n",
        "# -----------------------------\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Email Extraction (with constraints)\n",
        "# -----------------------------\n",
        "email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "emails = re.findall(email_pattern, text)\n",
        "for email in emails:\n",
        "    if len(email) > 5 and \".\" in email.split(\"@\")[-1]:\n",
        "        extracted_data[\"Email\"].add(email.lower())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Experience Extraction from date patterns\n",
        "# -----------------------------\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Merge overlapping experience durations\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Calculate total experience duration\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Rule-Based Skill Matching\n",
        "# -----------------------------\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\", \"mern stack\",\n",
        "    \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Last Education Detection (latest mentioned college/university)\n",
        "# -----------------------------\n",
        "education_labels = [\"College Name\", \"University\", \"Institute\", \"School\", \"Education\"]\n",
        "education_entities = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in education_labels:\n",
        "        education_entities.append((ent.start_char, ent.text.strip()))\n",
        "\n",
        "if education_entities:\n",
        "    education_entities.sort()  # sort by text position\n",
        "    last_education = education_entities[-1][1]\n",
        "    extracted_data[\"Last Education\"] = {last_education}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Display Extracted Results\n",
        "# -----------------------------\n",
        "print(\"\\n---- 📄 Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(sorted(values)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVScXuBI08eP",
        "outputId": "81bc20fc-9ff8-4ed5-d50e-182ae28e1bf2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- 📄 Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD RAZA\n",
            "College Name:\n",
            "  - SRE-Majeed Govt. Degree College\n",
            "Intermediate (Pre-Engineering)\n",
            "Skills:\n",
            "  - Data Science\n",
            "  - Machine Learning\n",
            "  - Python\n",
            "  - data science\n",
            "  - machine learning\n",
            "Email:\n",
            "  - mrezasyed3@gmail.com\n",
            "Last Education:\n",
            "  - SRE-Majeed Govt. Degree College\n",
            "Intermediate (Pre-Engineering)\n",
            "Location:\n",
            "  - Karachi\n",
            "Total Experience:\n",
            "  - 0 years 0 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# ✅ Resume Information Extractor (NER + Email + Experience + Skills + Last Education)\n",
        "# ------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Load the trained spaCy NER model\n",
        "# -----------------------------\n",
        "nlp = spacy.load(\"output/model-best\")  # Update this path as needed\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Extract text from PDF resume\n",
        "# -----------------------------\n",
        "resume_path = \"/content/Abdullah_CV.pdf\"\n",
        "text = extract_text(resume_path)\n",
        "doc = nlp(text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Initialize data container\n",
        "# -----------------------------\n",
        "extracted_data = {\n",
        "    \"Name\": set(),\n",
        "    \"College Name\": set(),\n",
        "    # \"Location\": set(),\n",
        "    \"Skills\": set(),\n",
        "    \"Email\": set(),\n",
        "    \"Last Education\": set(),\n",
        "    \"Experience\": set(),\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 NER-Based Entity Extraction\n",
        "# -----------------------------\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in extracted_data:\n",
        "        extracted_data[label] = set()\n",
        "    extracted_data[label].add(ent.text.strip())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Email Extraction (with constraints)\n",
        "# -----------------------------\n",
        "email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "emails = re.findall(email_pattern, text)\n",
        "for email in emails:\n",
        "    if len(email) > 5 and \".\" in email.split(\"@\")[-1]:\n",
        "        extracted_data[\"Email\"].add(email.lower())\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Experience Extraction from date patterns\n",
        "# -----------------------------\n",
        "experience_pattern = re.findall(\n",
        "    r\"(.+?)\\s*\\(?([A-Za-z]+\\s+\\d{2,4})\\)?\\s*[–-]\\s*\\(?([A-Za-z]+\\s+\\d{2,4}|Present)\\)?\",\n",
        "    text\n",
        ")\n",
        "\n",
        "raw_date_ranges = []\n",
        "for job_title, start_str, end_str in experience_pattern:\n",
        "    try:\n",
        "        start_date = parser.parse(start_str, default=datetime(1900, 1, 1))\n",
        "        end_date = datetime.today() if \"Present\" in end_str else parser.parse(end_str, default=datetime.today())\n",
        "        raw_date_ranges.append((start_date, end_date))\n",
        "        extracted_data[\"Experience\"].add(\n",
        "            f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Merge overlapping experience durations\n",
        "raw_date_ranges.sort()\n",
        "merged_ranges = []\n",
        "for current_start, current_end in raw_date_ranges:\n",
        "    if not merged_ranges:\n",
        "        merged_ranges.append((current_start, current_end))\n",
        "    else:\n",
        "        last_start, last_end = merged_ranges[-1]\n",
        "        if current_start <= last_end:\n",
        "            merged_ranges[-1] = (last_start, max(last_end, current_end))\n",
        "        else:\n",
        "            merged_ranges.append((current_start, current_end))\n",
        "\n",
        "# Calculate total experience duration\n",
        "total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged_ranges)\n",
        "years, months = divmod(total_months, 12)\n",
        "extracted_data[\"Total Experience\"] = {f\"{years} years {months} months\"}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Rule-Based Skill Matching\n",
        "# -----------------------------\n",
        "matcher = Matcher(nlp.vocab)\n",
        "skills = [\n",
        "    \"python\", \"javascript\", \"machine learning\", \"data science\", \"mern stack\",\n",
        "    \"react\", \"firebase\", \"fast api\", \"solidity\", \"postgresql\"\n",
        "]\n",
        "for skill in skills:\n",
        "    pattern = [{\"LOWER\": token} for token in skill.split()]\n",
        "    matcher.add(\"SKILLS\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    extracted_data[\"Skills\"].add(doc[start:end].text)\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Last Education Detection (latest mentioned college/university)\n",
        "# -----------------------------\n",
        "education_labels = [\"College Name\", \"University\", \"Institute\", \"School\", \"Education\"]\n",
        "education_entities = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in education_labels:\n",
        "        education_entities.append((ent.start_char, ent.text.strip()))\n",
        "\n",
        "if education_entities:\n",
        "    education_entities.sort()  # sort by text position\n",
        "    last_education = education_entities[-1][1]\n",
        "    extracted_data[\"Last Education\"] = {last_education}\n",
        "\n",
        "# -----------------------------\n",
        "# 🔹 Display Extracted Results\n",
        "# -----------------------------\n",
        "print(\"\\n---- 📄 Extracted Resume Information ----\")\n",
        "for key, values in extracted_data.items():\n",
        "    if values:\n",
        "        print(f\"{key}:\\n  - \" + \"\\n  - \".join(sorted(values)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CmqUyHz1pg9",
        "outputId": "f27d99f3-85db-4b7b-ce8e-33f1da679223"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- 📄 Extracted Resume Information ----\n",
            "Name:\n",
            "  - MUHAMMAD ABDULLAH\n",
            "College Name:\n",
            "  - Government Degree College SRE-Majeed, Karachi\n",
            "\n",
            "2017/April\n",
            "2019/July\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "Matriculation - Science\n",
            "-\n",
            "Computer Science\n",
            "\n",
            "Happy Palace Grammar School, Karachi\n",
            "\n",
            "Machine Learning Object Detection\n",
            "\n",
            "GitHub Link\n",
            "The model is a real-time image classification system designed to predict the class of objects (e.g., fruits\n",
            "and vegetables) captured through a live video feed from a webcam. It uses a pre-trained deep learning\n",
            "model loaded from a .h5 file to classify images into predefined classes.\n",
            "\n",
            "Power BI\n",
            "\n",
            "HR Dashboard\n",
            "A basic analysis of the employees of the company.\n",
            "\n",
            "Machine Learning Stock Prediction\n",
            "\n",
            "A basic prediction of stocks, using Linear Regression.\n",
            "\n",
            "EXPERIENCE\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "Feb 2025 – Present Project Manager Intern\n",
            "\n",
            "Binate Digital\n",
            "Assisting in managing software development projects, coordinating with cross-functional teams, and en-\n",
            "suring timely delivery of project milestones.\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "February/2024\n",
            "\n",
            "Data Analyst in Power BI Certificate\n",
            "\n",
            "September/2023\n",
            "\n",
            "IBM Data Science Certificate\n",
            "\n",
            "ACTIVITIES\n",
            "\n",
            "2022-2023\n",
            "\n",
            "Member of GDSC\n",
            "Was a member.\n",
            "\n",
            "DataCamp\n",
            "\n",
            "Coursera\n",
            "\n",
            "FEST, Hamdard University\n",
            "  - Hamdard University\n",
            "Skills:\n",
            "  - Data Science\n",
            "  - Machine Learning\n",
            "  - Python\n",
            "  - data science\n",
            "Email:\n",
            "  - itsmabdullah101@gmail.com\n",
            "Last Education:\n",
            "  - Government Degree College SRE-Majeed, Karachi\n",
            "\n",
            "2017/April\n",
            "2019/July\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "Matriculation - Science\n",
            "-\n",
            "Computer Science\n",
            "\n",
            "Happy Palace Grammar School, Karachi\n",
            "\n",
            "Machine Learning Object Detection\n",
            "\n",
            "GitHub Link\n",
            "The model is a real-time image classification system designed to predict the class of objects (e.g., fruits\n",
            "and vegetables) captured through a live video feed from a webcam. It uses a pre-trained deep learning\n",
            "model loaded from a .h5 file to classify images into predefined classes.\n",
            "\n",
            "Power BI\n",
            "\n",
            "HR Dashboard\n",
            "A basic analysis of the employees of the company.\n",
            "\n",
            "Machine Learning Stock Prediction\n",
            "\n",
            "A basic prediction of stocks, using Linear Regression.\n",
            "\n",
            "EXPERIENCE\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "GitHub Link\n",
            "\n",
            "Feb 2025 – Present Project Manager Intern\n",
            "\n",
            "Binate Digital\n",
            "Assisting in managing software development projects, coordinating with cross-functional teams, and en-\n",
            "suring timely delivery of project milestones.\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "February/2024\n",
            "\n",
            "Data Analyst in Power BI Certificate\n",
            "\n",
            "September/2023\n",
            "\n",
            "IBM Data Science Certificate\n",
            "\n",
            "ACTIVITIES\n",
            "\n",
            "2022-2023\n",
            "\n",
            "Member of GDSC\n",
            "Was a member.\n",
            "\n",
            "DataCamp\n",
            "\n",
            "Coursera\n",
            "\n",
            "FEST, Hamdard University\n",
            "Experience:\n",
            "  - GitHub Link (Feb 2025 - Jul 2025)\n",
            "Location:\n",
            "  - Hamdard\n",
            "  - Karachi\n",
            "Degree:\n",
            "  - Bachelor’s student majoring in Computer Science\n",
            "Total Experience:\n",
            "  - 0 years 5 months\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuzySYdtLGao"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# import re\n",
        "# from pdfminer.high_level import extract_text\n",
        "# from dateutil import parser\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Load the trained NER model\n",
        "# nlp = spacy.load(\"output/model-last\")\n",
        "\n",
        "# # Extract text from the resume\n",
        "# text = extract_text(\"/content/Muhammad Fareed Full Stack Developer Resume - Muhammad Fareed.pdf\")\n",
        "# doc = nlp(text)\n",
        "\n",
        "# # Store extracted entities in a dictionary (Use Sets to Avoid Duplicates)\n",
        "# extracted_data = {\n",
        "#     \"Name\": set(),\n",
        "#     \"Degree\": set(),\n",
        "#     \"College Name\": set(),\n",
        "#     \"Location\": set(),\n",
        "#     \"Skills\": set(),\n",
        "#     \"Experience\": set(),\n",
        "# }\n",
        "\n",
        "# # Extract Named Entities (NER)\n",
        "# for ent in doc.ents:\n",
        "#     extracted_data[ent.label_].add(ent.text.strip())\n",
        "\n",
        "# # Remove unwanted terms from Skills & Degree\n",
        "# invalid_terms = {\"SKILLS\", \"SUMMARY\", \"WORK EXPERIENCE\", \"EDUCATION\"}\n",
        "# extracted_data[\"Degree\"] -= invalid_terms\n",
        "# extracted_data[\"Skills\"] -= invalid_terms\n",
        "\n",
        "# # Extract skills manually (if missing from NER)\n",
        "# skills_section = re.findall(\n",
        "#     r\"(?i)(?:Skills|Technical Skills|Expertise|SKILLS SUMMARY)[\\s\\S]*?(?=\\n\\n|\\Z)\", text\n",
        "# )\n",
        "# if skills_section:\n",
        "#     skills = re.findall(r\"\\b[A-Za-z#.\\-+]+\\b\", skills_section[0])\n",
        "#     extracted_data[\"Skills\"].update(skill for skill in skills if skill.upper() not in invalid_terms)\n",
        "\n",
        "# # Extract job experience details (using regex)\n",
        "# experience_pattern = re.findall(\n",
        "#     r\"([\\w\\s\\-/]+)\\s*[\\n]*\\s*(\\w+\\s*\\d{4})\\s*–\\s*(\\w+\\s*\\d{4}|Present)\", text\n",
        "# )\n",
        "\n",
        "# total_experience_months = 0\n",
        "# for job_title, start_date, end_date in experience_pattern:\n",
        "#     try:\n",
        "#         start_date = parser.parse(start_date)\n",
        "#         end_date = (\n",
        "#             parser.parse(end_date) if \"Present\" not in end_date else datetime.today()\n",
        "#         )\n",
        "#         experience_months = (end_date.year - start_date.year) * 12 + (\n",
        "#             end_date.month - start_date.month\n",
        "#         )\n",
        "#         total_experience_months += experience_months\n",
        "\n",
        "#         extracted_data[\"Experience\"].add(\n",
        "#             f\"{job_title.strip()} ({start_date.strftime('%b %Y')} - {end_date.strftime('%b %Y')})\"\n",
        "#         )\n",
        "#     except Exception:\n",
        "#         continue  # Skip parsing errors\n",
        "\n",
        "# # Convert total experience to years & months\n",
        "# if total_experience_months > 0:\n",
        "#     years, months = divmod(total_experience_months, 12)\n",
        "#     extracted_data[\"Total Experience\"] = f\"{years} years {months} months\"\n",
        "\n",
        "# # Remove duplicates & format results properly\n",
        "# for key in extracted_data:\n",
        "#     extracted_data[key] = list(set(extracted_data[key]))  # Convert sets to lists\n",
        "\n",
        "# # Display the cleaned-up results\n",
        "# print(\"\\n---- Extracted Information ----\")\n",
        "# for key, values in extracted_data.items():\n",
        "#     if values:\n",
        "#          print(f\"{key}: {' '.join(values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xZi0d1e39ms"
      },
      "outputs": [],
      "source": [
        "# print(\"---- Full Resume Text ----\")\n",
        "# print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qO5hHqnpsIg"
      },
      "outputs": [],
      "source": [
        "# import sys , fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHKQp7vtpsEV"
      },
      "outputs": [],
      "source": [
        "# fname = '/content/CV-Parsing-using-Spacy-3/data/test/Muhammad Raza Resume (1).pdf'\n",
        "# doc = fitz.open(fname)\n",
        "# text = \"\"\n",
        "# for page in doc:\n",
        "  # text = text + str(page.get_text())\n",
        "# doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjIxwf1YpsCy"
      },
      "outputs": [],
      "source": [
        "# text = text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy1-dVU8pr-3"
      },
      "outputs": [],
      "source": [
        "# txt = \" \".join(text.split('\\n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvQw2UishtH0"
      },
      "outputs": [],
      "source": [
        "# print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7d36gGPpr9h"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(text)\n",
        "# for ent in doc.ents:\n",
        "  # print(ent.text, '---->' , ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "id": "lD2teLSpprzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ec8777-e327-4254-8b0d-1e34be50d96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/model-last/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-last/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-last/ner/ (stored 0%)\n",
            "  adding: content/output/model-last/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-last/ner/moves (deflated 74%)\n",
            "  adding: content/output/model-last/ner/model (deflated 8%)\n",
            "  adding: content/output/model-last/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-last/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-last/vocab/ (stored 0%)\n",
            "  adding: content/output/model-last/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-last/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-last/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-last/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-last/meta.json (deflated 66%)\n",
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-best/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 74%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/meta.json (deflated 66%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/output.zip /content/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M2ZJLvBs5ic"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy debug data config.cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzQ1mwz6YIwP"
      },
      "source": [
        "---\n",
        "<h4>For Reference:</h4>\n",
        "<a href='https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git'>CV-Parsing-using-Spacy-3\n",
        "</a><br>\n",
        "<a href='https://github.com/yashlikescode/spacyResumeParcer.git'>spacyResumeParcer</a>\n",
        "<hr>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}